# ActCLI (prototype)

ActCLI is a terminalâ€‘native toolkit for actuarial workflows with a multiâ€‘model "roundtable" chat featuring a **VSCode-style interface**. This scaffold demonstrates the WOW path: concurrent responses from multiple models, a second round that references peers, and a simple synthesis. It also includes a firstâ€‘run doctor and stubbed auth.

## ðŸŽ¨ User Interface

ActCLI features a professional **VSCode-style terminal interface** with:

- **Left Sidebar (25%)**: Expandable sections for models, themes, MCP servers, locations
- **ASCII Art Logo**: "ActCLI - Actuarial CLI"
- **Right Chat Area (75%)**: Conversation history and input
- **Theme Switching**: Dark, Light, Nord themes (Ctrl+T)
- **Model Management**: See available vs active models in roundtable
- **Keyboard Navigation**: Arrow keys, text selection, copy functionality

Start the interactive interface with:
```bash
actcli  # Enters VSCode-style chat interface
```

Or choose specific layouts:
```bash
ACTCLI_LAYOUT=vscode actcli    # VSCode-style (default)
ACTCLI_LAYOUT=claude actcli    # Claude CLI-style
ACTCLI_LAYOUT=basic actcli     # Basic terminal
```

## Quickstart

- Python 3.11+
- Create venv and install:
  - `python -m venv .venv && source .venv/bin/activate`
  - `pip install -e .`
- Health check:
  - `actcli doctor`
- Roundtable demo:
  - `actcli chat --prompt "Compare reserving strategies" --multi llama3,claude,gpt --rounds 2`
- Auth status (API keys via env):
  - `actcli auth status`

## Layout (scaffold)
- `src/actcli/cli.py` â€” Typer entry (`actcli`)
- `src/actcli/commands/` â€” `chat`, `doctor`, `auth`
- `src/actcli/seminar/` â€” adapters, coordinator, synthesizer
- `src/actcli/auth/` â€” provider registry and credential store

Notes
- Adapters currently use an `EchoAdapter` (no network) to validate concurrency and UX. Real providers (OpenAI/Anthropic/Google, Ollama) can be plugged in next.
- `doctor` prints a concise readiness summary to build user confidence on first run.
- Auth supports API key envs now; OAuth device/PKCE will be added via provider plugins.

## Docs

- Start here: `docs/README.md`
- Task checklist: `docs/TASKS.md`
- **UI Layouts**: `docs/LAYOUTS.md` - VSCode-style interface guide
- **UI Implementation**: `docs/UI_IMPROVEMENTS.md` - Technical details

## Local models in this repo

Run a project-scoped Ollama server that stores models under `./models`:

- Start server: `scripts/ollama-local.sh serve` (port 11435; models in `./models`)
- Pull defaults: `scripts/ollama-local.sh pull-all` (codellama:34b, gpt-oss:20b, codellama:13b, llama3:8b, llama3.2:3b)
- List via CLI: `actcli models list --ollama-host http://127.0.0.1:11435`
- Use in chat: `actcli chat --multi "codellama:34b,gpt-oss:20b,codellama:13b" --rounds 2 --ollama-host http://127.0.0.1:11435`

### One-shot runner (fresh start + action)

- Fresh reset and pull all defaults:
  - `scripts/fresh.sh pull-all`
- Fresh reset and run a chat:
  - `scripts/fresh.sh chat --prompt "Compare approaches" --multi "codellama:34b,gpt-oss:20b,codellama:13b"`
- Fresh reset and list models:
  - `scripts/fresh.sh models list`
